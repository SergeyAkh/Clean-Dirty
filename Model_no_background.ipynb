{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdeac93c-2b2b-48c9-8919-6aab4beb3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5b8e9c-aaf6-4929-9210-f892cd634f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d463d0bd-a341-4191-9c7a-ccb24968890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"cleaned\", \"dirty\"]\n",
    "train_path = \"/Users/sergeiakhmadulin/Documents/No_background/train/\"\n",
    "val_path = \"/Users/sergeiakhmadulin/Documents/No_background/valid/\"\n",
    "test_path = \"/Users/sergeiakhmadulin/Documents/No_background/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0b72b78d-d0e8-4055-af4a-ac961f775c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in train folder of class cleaned: 20\n",
      "Files in train folder of class dirty: 20\n"
     ]
    }
   ],
   "source": [
    "for class_name in class_names:\n",
    "    full_path = train_path + class_name + \"/\"\n",
    "    print(f\"Files in train folder of class {class_name}: {len(list(filter(lambda x: x.endswith('.jpg'), os.listdir(full_path))))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5074a86a-e29e-4621-a5d3-5f7a8c174ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRemoveBackground:\n",
    "    \"\"\"Remove images background.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, in_img):\n",
    "        \n",
    "        # Convert PIL image to numpy array\n",
    "        in_img = np.array(in_img)\n",
    "        \n",
    "        # Get the height and width from OpenCV image\n",
    "        height, width = in_img.shape[:2]\n",
    "        \n",
    "        # Create a mask holder\n",
    "        mask = np.zeros([height, width], np.uint8)\n",
    "\n",
    "        # Grab Cut the object\n",
    "        bgdModel = np.zeros((1, 65),np.float64)\n",
    "        fgdModel = np.zeros((1, 65),np.float64)\n",
    "\n",
    "        # Hard Coding the Rect The object must lie within this rect.\n",
    "        rect = (15, 15, width-30, height-30)\n",
    "        cv2.grabCut(in_img, mask, rect, bgdModel, fgdModel, 10, cv2.GC_INIT_WITH_RECT)\n",
    "        mask = np.where((mask==2)|(mask==0), 0, 1).astype('uint8')\n",
    "        out_img = in_img * mask[:, :, np.newaxis]\n",
    "\n",
    "        # Get the background\n",
    "        background = in_img - out_img\n",
    "\n",
    "        # Change all pixels in the background that are not black to white\n",
    "        background[np.where((background > [0, 0, 0]).all(axis = 2))] = [255, 255, 255]\n",
    "\n",
    "        #Add the background and the image\n",
    "        out_img = background + out_img\n",
    "\n",
    "        return Image.fromarray(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb6bca1-070c-49eb-a34f-39b693c8d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_background(image_roots):\n",
    "    \"\"\"Remove picture background.\n",
    "       This function use MyRemoveBackground class.\n",
    "    \"\"\"\n",
    "    remove_photo_background = MyRemoveBackground()\n",
    "\n",
    "    print('Backgrounds removing started...')\n",
    "    for path in image_roots:\n",
    "        files = os.listdir(path)\n",
    "        files = list(filter(lambda x: x.endswith('.jpg'), files))\n",
    "        \n",
    "        print(f'{len(files)} pictures was found in {path}', end='')\n",
    "        for i, file in enumerate(files):\n",
    "            img_original = cv2.imread(path + file)\n",
    "            img_cleaned = remove_photo_background(img_original)\n",
    "            img_cleaned = np.array(img_cleaned)\n",
    "            cv2.imwrite(path + file, img_cleaned)\n",
    "            if i % 20 == 0:\n",
    "                print('\\n{:>3d}/{:>3d}'.format(i, len(files)), end='')\n",
    "            print('.', end='')\n",
    "        print()\n",
    "    print('Backgrounds removing is complete.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e8555f9-aded-4358-b939-095c3f6ac9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgrounds removing started...\n",
      "20 pictures was found in /Users/sergeiakhmadulin/Documents/No_background/train/cleaned/\n",
      "  0/ 20....................\n",
      "20 pictures was found in /Users/sergeiakhmadulin/Documents/No_background/train/dirty/\n",
      "  0/ 20....................\n",
      "Backgrounds removing is complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_background(image_roots=[os.path.join(train_path, 'cleaned/'),\n",
    "                               os.path.join(train_path, 'dirty/')\n",
    "#                                ,os.path.join(data_root, 'test/')\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8dc2b193-6961-4233-9d7b-a85d298f5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_random_files(source_folder, destination_folder, ratio_for_val):\n",
    "    # Get all files in the source folder\n",
    "    for class_name in class_names:\n",
    "        full_path_source = source_folder + class_name + \"/\"\n",
    "        full_destination_folder = destination_folder + class_name + \"/\"\n",
    "        os.makedirs(full_destination_folder, exist_ok=True)\n",
    "        files = os.listdir(full_path_source)\n",
    "        files = list(filter(lambda x: x.endswith('.jpg'), files))\n",
    "        \n",
    "        whole_part = int(len(files) * ratio_for_val)\n",
    "        # Randomly select the specified number of files\n",
    "        if len(files) < whole_part:\n",
    "            print(f\"Not enough files to choose from. Available: {len(files)}\")\n",
    "            return\n",
    "    \n",
    "        selected_files = random.sample(files, whole_part)\n",
    "        # Move each selected file to the destination folder\n",
    "        for file_name in selected_files:\n",
    "            shutil.move(os.path.join(full_path_source, file_name), os.path.join(full_destination_folder, file_name))\n",
    "    print(f'Moved to validation folder: {len(selected_files)} files of each class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "39bc9d85-a63a-4936-9653-9c57adfd3d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sergeiakhmadulin/Documents/No_background/valid/cleaned/\n",
      "Moved to validation folder: 0011.jpg\n",
      "Moved to validation folder: 0017.jpg\n",
      "Moved to validation folder: 0004.jpg\n",
      "Moved to validation folder: 0015.jpg\n",
      "/Users/sergeiakhmadulin/Documents/No_background/valid/dirty/\n",
      "Moved to validation folder: 0005.jpg\n",
      "Moved to validation folder: 0018.jpg\n",
      "Moved to validation folder: 0003.jpg\n",
      "Moved to validation folder: 0012.jpg\n"
     ]
    }
   ],
   "source": [
    "move_random_files(train_path, val_path, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b8fd1e2e-5f37-4daa-ba49-f8b9e70986c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "afaeb31b-3397-41cd-9155-cc2fe72e7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(image):\n",
    "    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "    noisy_image = image + noise\n",
    "    return noisy_image\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Randomly choose a transformation\n",
    "    transform_choice = np.random.choice(['none', 'noise', 'grayscale', 'brightness'])\n",
    "    if transform_choice == 'noise':\n",
    "        image = add_gaussian_noise(image)\n",
    "    elif transform_choice == 'grayscale':\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "    elif transform_choice == 'brightness':\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "30e110ad-e42e-41fa-baa3-ecd67045c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,    \n",
    "    rotation_range=90,\n",
    "    vertical_flip=True,\n",
    "    channel_shift_range = 0.3,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7466f3cc-7c90-4ae2-9077-d32968d750d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augm_data(datagen_function, folder, num_of_img, classes):\n",
    "    print(\"Augmantation started.\")\n",
    "    for class_name in classes:\n",
    "        full_path = folder + class_name + \"/\"\n",
    "        original_images = []  # List to hold your original images\n",
    "        files = os.listdir(full_path)\n",
    "        files = list(filter(lambda x: x.endswith('.jpg'), files))\n",
    "        for num_img, filename in enumerate(files):\n",
    "            img = Image.open(os.path.join(full_path, filename))\n",
    "            original_images.append(np.array(img))\n",
    "        num_of_original_images = len(original_images)\n",
    "        print(f\"\\nFound original images of class {class_name}: {num_of_original_images}\")\n",
    "        num_of_each_img = num_of_img//num_of_original_images\n",
    "        num_of_new = 0\n",
    "        for num_orig, img in enumerate(original_images):\n",
    "            img = img.reshape((1,) + img.shape)  # Reshape for the generator\n",
    "            current_count = 0\n",
    "            for num_augm, batch in enumerate(datagen.flow(img, batch_size=1)):\n",
    "                augmented_image = batch[0].astype(np.uint8)\n",
    "                # Save or process your augmented images\n",
    "                Image.fromarray(augmented_image).save(full_path + f'augmented_image_{num_orig}_{num_augm}.jpg')\n",
    "                current_count += 1\n",
    "                num_of_new += 1\n",
    "                if current_count >= num_of_each_img:\n",
    "                    break\n",
    "        print(f\"Add new augmented images of class {class_name}: {num_of_new}\")\n",
    "    print(\"\\nAugmentation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e881f53d-d220-4214-b14a-1755ed336dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmantation started.\n",
      "\n",
      "Found original images of class cleaned: 16\n",
      "Add new augmented images of class cleaned: 30000\n",
      "\n",
      "Found original images of class dirty: 16\n",
      "Add new augmented images of class dirty: 30000\n",
      "\n",
      "Augmentation finished.\n"
     ]
    }
   ],
   "source": [
    "generate_augm_data(datagen, train_path, 30000, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "18c75cf2-f276-43fb-a235-9ce59dafe574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmantation started.\n",
      "\n",
      "Found original images of class cleaned: 4\n",
      "Add new augmented images of class cleaned: 700\n",
      "\n",
      "Found original images of class dirty: 4\n",
      "Add new augmented images of class dirty: 700\n",
      "\n",
      "Augmentation finished.\n"
     ]
    }
   ],
   "source": [
    "generate_augm_data(datagen, val_path, 700, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d4a063fb-55ed-4931-a71e-bd685ea961c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_datasets():\n",
    "    \"\"\"Creates datasets for training and validation.\n",
    "\n",
    "    Returns:\n",
    "        (tf.data.Dataset, tf.data.Dataset): Training and validation datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    training_dataset = tf.keras.utils.image_dataset_from_directory( \n",
    "        directory=train_path,\n",
    "        batch_size=16,\n",
    "        image_size=(224, 224),\n",
    "        shuffle=True, \n",
    "    ) \n",
    "    \n",
    "    validation_dataset = tf.keras.utils.image_dataset_from_directory( \n",
    "        directory=val_path,\n",
    "        batch_size=16,\n",
    "        image_size=(224, 224),\n",
    "        shuffle=True, \n",
    "    ) \n",
    "\n",
    "    return training_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "adf36c91-65d4-4eb1-9760-9f5a323e088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60032 files belonging to 2 classes.\n",
      "Found 1408 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_dataset, validation_dataset = train_val_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44a76881-bca2-45d7-9e9c-8a47ea3718e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_WEIGHTS_FILE = '/Users/sergeiakhmadulin/My Drive/Clean-Dirty/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "484314c0-63da-466e-8f99-3ba5e3ac7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pre_trained_model():\n",
    "    \"\"\"Creates the pretrained inception V3 model\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: pre-trained model\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    pre_trained_model = tf.keras.applications.EfficientNetB0( \n",
    "        include_top=False, \n",
    "        input_shape=(224, 224, 3),\n",
    "        weights='imagenet'\n",
    "    ) \n",
    "\n",
    "    # Make all the layers in the pre-trained model non-trainable\n",
    "    # pre_trained_model.load_weights(LOCAL_WEIGHTS_FILE)\n",
    "    pre_trained_model.trainable = False\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return pre_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "afaee367-5eca-4bed-ad25-07d2ffad3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = create_pre_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "24bdc4a8-f9f4-4e44-9a29-e96bd5c5af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation='sigmoid')  # For binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3601e88a-ad2f-472a-970b-4477d1297b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_8      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_8      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │         \u001b[38;5;34m5,120\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,055,972</span> (15.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,055,972\u001b[0m (15.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,841</span> (15.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,841\u001b[0m (15.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,052,131</span> (15.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,052,131\u001b[0m (15.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "27cdbd13-6130-46f1-9eb9-dc532a81c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['accuracy'] >= 0.99 and logs['val_accuracy'] >= 0.99:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "            print(\"\\nReached 80% train accuracy and 80% validation accuracy, so cancelling training!\")\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.keras',               # File name to save the model\n",
    "    monitor='val_accuracy',         # Metric to monitor\n",
    "    save_best_only=True,           # Save only the best model\n",
    "    mode='max',                    # Mode (max for accuracy)\n",
    "    verbose=1                      # Verbosity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2211238d-824e-45dd-be99-11d9f39197d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cdaf43c1-5f47-474c-8ee2-86147cb745ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 150ms/step - accuracy: 0.8654 - loss: 0.3043 - val_accuracy: 0.7038 - val_loss: 1.0416\n",
      "Epoch 2/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 151ms/step - accuracy: 0.9694 - loss: 0.0886 - val_accuracy: 0.7109 - val_loss: 1.2013\n",
      "Epoch 3/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 149ms/step - accuracy: 0.9788 - loss: 0.0638 - val_accuracy: 0.7131 - val_loss: 1.3088\n",
      "Epoch 4/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 151ms/step - accuracy: 0.9802 - loss: 0.0567 - val_accuracy: 0.7124 - val_loss: 1.3807\n",
      "Epoch 5/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 147ms/step - accuracy: 0.9819 - loss: 0.0515 - val_accuracy: 0.7173 - val_loss: 1.4168\n",
      "Epoch 6/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 158ms/step - accuracy: 0.9827 - loss: 0.0508 - val_accuracy: 0.7195 - val_loss: 1.4037\n",
      "Epoch 7/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m596s\u001b[0m 159ms/step - accuracy: 0.9829 - loss: 0.0496 - val_accuracy: 0.7195 - val_loss: 1.4524\n",
      "Epoch 8/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 160ms/step - accuracy: 0.9836 - loss: 0.0459 - val_accuracy: 0.7209 - val_loss: 1.4224\n",
      "Epoch 9/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m596s\u001b[0m 159ms/step - accuracy: 0.9841 - loss: 0.0448 - val_accuracy: 0.7188 - val_loss: 1.4735\n",
      "Epoch 10/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m599s\u001b[0m 160ms/step - accuracy: 0.9846 - loss: 0.0431 - val_accuracy: 0.7209 - val_loss: 1.4993\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_dataset,\n",
    "    validation_data = validation_dataset,\n",
    "    epochs = 10,\n",
    "    verbose = 1,\n",
    "    callbacks = [EarlyStoppingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b74d1787-ee6d-4d86-a51f-aacb14dd2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:100]:  # Freeze first 100 layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile and train the model again\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f569e248-2e64-488b-82b1-ee95359885f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_7      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_7      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │         \u001b[38;5;34m5,120\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,055,972</span> (15.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,055,972\u001b[0m (15.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,054,545</span> (7.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,054,545\u001b[0m (7.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,001,427</span> (7.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,001,427\u001b[0m (7.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ce6ce2ea-6213-4df9-8f31-9d942e5d12d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1296s\u001b[0m 344ms/step - accuracy: 0.9750 - loss: 0.0662 - val_accuracy: 0.7202 - val_loss: 1.7308\n",
      "Epoch 2/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1197s\u001b[0m 319ms/step - accuracy: 0.9938 - loss: 0.0167 - val_accuracy: 0.7379 - val_loss: 1.9163\n",
      "Epoch 3/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1139s\u001b[0m 304ms/step - accuracy: 0.9968 - loss: 0.0093 - val_accuracy: 0.8310 - val_loss: 1.3866\n",
      "Epoch 4/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1106s\u001b[0m 295ms/step - accuracy: 0.9970 - loss: 0.0075 - val_accuracy: 0.7266 - val_loss: 1.4376\n",
      "Epoch 5/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1110s\u001b[0m 296ms/step - accuracy: 0.9969 - loss: 0.0078 - val_accuracy: 0.8054 - val_loss: 1.6076\n",
      "Epoch 6/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1152s\u001b[0m 307ms/step - accuracy: 0.9979 - loss: 0.0052 - val_accuracy: 0.7614 - val_loss: 1.5370\n",
      "Epoch 7/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1180s\u001b[0m 314ms/step - accuracy: 0.9975 - loss: 0.0062 - val_accuracy: 0.7564 - val_loss: 1.4362\n",
      "Epoch 8/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1160s\u001b[0m 309ms/step - accuracy: 0.9980 - loss: 0.0041 - val_accuracy: 0.7301 - val_loss: 1.8494\n",
      "Epoch 9/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1141s\u001b[0m 304ms/step - accuracy: 0.9979 - loss: 0.0053 - val_accuracy: 0.7649 - val_loss: 1.2975\n",
      "Epoch 10/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1144s\u001b[0m 305ms/step - accuracy: 0.9982 - loss: 0.0046 - val_accuracy: 0.7024 - val_loss: 2.0965\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_dataset,\n",
    "    validation_data = validation_dataset,\n",
    "    epochs = 10,\n",
    "    verbose = 1,\n",
    "    callbacks = [EarlyStoppingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "65330ed0-b1c2-4c4b-8436-e61148f773a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size):\n",
    "    remove_photo_background = MyRemoveBackground()\n",
    "    img = load_img(image_path, target_size=(224,224))  # Resize to match model's input shape\n",
    "    img_array = remove_photo_background(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c560438d-f9a4-41bf-a4a2-5b0aac048f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "files = list(filter(lambda x: x.endswith('.jpg'), os.listdir(test_path)))\n",
    "for filename in files:\n",
    "    image_path = os.path.join(test_path, filename)\n",
    "    processed_image = preprocess_image(image_path, target_size)\n",
    "\n",
    "    # Predict the label\n",
    "    prediction = model.predict(processed_image, verbose=0)\n",
    "    \n",
    "    # Get the predicted class (for example, using argmax for multi-class classification)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "    results.append({'id': filename, 'label': predicted_class, 'probs': prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ad8cf945-96bc-47aa-bc7f-1f0d5d9684ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_test = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "644fbf4d-a275-4765-888c-e7a1aed2bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_test[\"dirty\"] = np.where(results_df_test[\"probs\"]>=0.99,1,0)\n",
    "results_df_test[\"cleaned\"] = np.where(results_df_test[\"probs\"]<=0.01,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f9e90891-5cff-421d-b795-0df311fb13d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = results_df_test[results_df_test[\"cleaned\"]==1]\n",
    "id_cleaned = list(df_cleaned[\"id\"])\n",
    "\n",
    "df_dirty = results_df_test[results_df_test[\"dirty\"]==1]\n",
    "id_dirty = list(df_dirty[\"id\"])\n",
    "\n",
    "min_value = min(len(id_cleaned), len(id_dirty))\n",
    "\n",
    "index_to_save = random.sample(range(min_value), min_value)\n",
    "\n",
    "files_clean_to_stay = list(df_cleaned.iloc[index_to_save][\"id\"])\n",
    "files_dirty_to_stay = list(df_dirty.iloc[index_to_save][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7573cdc8-0c7e-41e1-ada1-df431e59ae7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d1f507c2-c4af-4a65-8002-506b20745367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "52708eb3-9b33-4a1f-b2a5-10bc82c3985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_dir_with_files_to_train(source_folder, destination_folder, \n",
    "                                     list_given_file_names_cleaned, list_given_file_names_dirty):\n",
    "\n",
    "    # Get all files in the source folder\n",
    "    for class_name in class_names:\n",
    "        full_path_source = source_folder + \"/\"\n",
    "        full_destination_folder = destination_folder + class_name + \"/\"\n",
    "        os.makedirs(full_destination_folder, exist_ok=True)\n",
    "        \n",
    "        if class_name == \"cleaned\":\n",
    "            matched_files_cleaned = [file for file in os.listdir(full_path_source) if file in list_given_file_names_cleaned]\n",
    "            for file_name in matched_files_cleaned:\n",
    "                shutil.copy(os.path.join(full_path_source, file_name), os.path.join(full_destination_folder, file_name))\n",
    "\n",
    "        else:\n",
    "            matched_files = [file for file in os.listdir(full_path_source) if file in list_given_file_names_dirty]\n",
    "            for file_name in matched_files:\n",
    "                shutil.copy(os.path.join(full_path_source, file_name), os.path.join(full_destination_folder, file_name))\n",
    "            \n",
    "    print(f'Copied files from test to new folder to train: {len(list_given_file_names_dirty)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a2420821-ebe9-4e6e-8e42-5ee18c36b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_new_files_train = \"/Users/sergeiakhmadulin/Documents/new_files/train/\"\n",
    "path_new_files_val = \"/Users/sergeiakhmadulin/Documents/new_files/valid/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a80ae921-9a39-4c31-9018-eb0a1639ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied files from test to new folder to train: 70\n"
     ]
    }
   ],
   "source": [
    "make_new_dir_with_files_to_train(test_path, path_new_files_train, \n",
    "                                     files_clean_to_stay, files_dirty_to_stay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c5ce39d3-daec-49dc-95cf-8d4ffec52b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgrounds removing started...\n",
      "70 pictures was found in /Users/sergeiakhmadulin/Documents/new_files/train/cleaned/\n",
      "  0/ 70....................\n",
      " 20/ 70....................\n",
      " 40/ 70....................\n",
      " 60/ 70..........\n",
      "70 pictures was found in /Users/sergeiakhmadulin/Documents/new_files/train/dirty/\n",
      "  0/ 70....................\n",
      " 20/ 70....................\n",
      " 40/ 70....................\n",
      " 60/ 70..........\n",
      "Backgrounds removing is complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_background(image_roots=[os.path.join(path_new_files_train, 'cleaned/'),\n",
    "                               os.path.join(path_new_files_train, 'dirty/')\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "265a5b14-1bf5-4582-b521-a0522fa2b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to validation folder: 14 files of each class\n"
     ]
    }
   ],
   "source": [
    "move_random_files(path_new_files_train, path_new_files_val, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "35fcfeac-cf3b-4e98-81c5-5227d5d08250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmantation started.\n",
      "\n",
      "Found original images of class cleaned: 56\n",
      "Add new augmented images of class cleaned: 29960\n",
      "\n",
      "Found original images of class dirty: 56\n",
      "Add new augmented images of class dirty: 29960\n",
      "\n",
      "Augmentation finished.\n"
     ]
    }
   ],
   "source": [
    "generate_augm_data(datagen, path_new_files_train, 30000, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "063c481e-3933-4d5f-a43d-7109e5eb8326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmantation started.\n",
      "\n",
      "Found original images of class cleaned: 14\n",
      "Add new augmented images of class cleaned: 700\n",
      "\n",
      "Found original images of class dirty: 14\n",
      "Add new augmented images of class dirty: 700\n",
      "\n",
      "Augmentation finished.\n"
     ]
    }
   ],
   "source": [
    "generate_augm_data(datagen, path_new_files_val, 700, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "00ac8ab0-5dde-4056-be75-41242725e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files_in_directory(directory, new_prefix):\n",
    "    # Loop through all files in the specified directory\n",
    "    for index, filename in enumerate(os.listdir(directory)):\n",
    "        # Create the full file path\n",
    "        old_file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Check if it is a file (not a directory)\n",
    "        if os.path.isfile(old_file_path):\n",
    "            # Create new file name with the specified prefix\n",
    "            new_filename = f\"{new_prefix}_{index + 1}{os.path.splitext(filename)[1]}\"\n",
    "            new_file_path = os.path.join(directory, new_filename)\n",
    "            \n",
    "            # Rename the file\n",
    "            os.rename(old_file_path, new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3aee57c3-4083-4a38-b93d-d4c42b3df1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_files_in_directory(path_new_files_train + \"cleaned\" + \"/\", \"_new\")\n",
    "rename_files_in_directory(path_new_files_train + \"dirty\" + \"/\", \"_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a84c5ee5-0d8e-4cc6-8fdc-6dc6d0664a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_files_in_directory(path_new_files_val + \"cleaned\" + \"/\", \"_new\")\n",
    "rename_files_in_directory(path_new_files_val + \"dirty\" + \"/\", \"_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3afe9c45-b988-4d88-b26b-e35e9a851251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(path_from, path_to, class_names):\n",
    "    for class_name in class_names:\n",
    "        full_sourse_folder = path_from + class_name + \"/\"\n",
    "        full_destination_folder = path_to + class_name + \"/\"\n",
    "        files_to_copy = list(filter(lambda x: x.endswith('.jpg'), os.listdir(full_sourse_folder)))\n",
    "        print(full_sourse_folder, full_destination_folder)\n",
    "        for filename in files_to_copy:\n",
    "            shutil.copy(os.path.join(full_sourse_folder, filename), full_destination_folder)\n",
    "        files_in_folder = len(list(filter(lambda x: x.endswith('.jpg'), os.listdir(full_destination_folder))))\n",
    "        print(f\"Copy fineshed. In folder {full_destination_folder} now: {files_in_folder} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e4b8bbba-273c-412f-8ffa-05554a204521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sergeiakhmadulin/Documents/new_files/train/cleaned/ /Users/sergeiakhmadulin/Documents/No_background/train/cleaned/\n",
      "Copy fineshed. In folder /Users/sergeiakhmadulin/Documents/No_background/train/cleaned/ now: 30016 files\n",
      "/Users/sergeiakhmadulin/Documents/new_files/train/dirty/ /Users/sergeiakhmadulin/Documents/No_background/train/dirty/\n",
      "Copy fineshed. In folder /Users/sergeiakhmadulin/Documents/No_background/train/dirty/ now: 30016 files\n"
     ]
    }
   ],
   "source": [
    "copy_files(path_new_files_train, train_path, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9c8dc3ef-f1a7-4494-8947-a27cedccf552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sergeiakhmadulin/Documents/new_files/valid/cleaned/ /Users/sergeiakhmadulin/Documents/No_background/train/cleaned/\n",
      "Copy fineshed. In folder /Users/sergeiakhmadulin/Documents/No_background/train/cleaned/ now: 714 files\n",
      "/Users/sergeiakhmadulin/Documents/new_files/valid/dirty/ /Users/sergeiakhmadulin/Documents/No_background/train/dirty/\n",
      "Copy fineshed. In folder /Users/sergeiakhmadulin/Documents/No_background/train/dirty/ now: 714 files\n"
     ]
    }
   ],
   "source": [
    "copy_files(path_new_files_val, train_path, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8572aabe-7deb-41a3-b368-48617aa163c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120072 files belonging to 2 classes.\n",
      "Found 2836 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_dataset, validation_dataset = train_val_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ed808d8c-fe39-4682-8543-9de3c24e0b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7505/7505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 125ms/step - accuracy: 0.9051 - loss: 0.2353 - val_accuracy: 0.8889 - val_loss: 0.2794\n",
      "Epoch 2/10\n",
      "\u001b[1m7505/7505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 122ms/step - accuracy: 0.9661 - loss: 0.0971 - val_accuracy: 0.8963 - val_loss: 0.2759\n",
      "Epoch 3/10\n",
      "\u001b[1m7505/7505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m922s\u001b[0m 123ms/step - accuracy: 0.9734 - loss: 0.0782 - val_accuracy: 0.9027 - val_loss: 0.2596\n",
      "Epoch 4/10\n",
      "\u001b[1m7505/7505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m918s\u001b[0m 122ms/step - accuracy: 0.9747 - loss: 0.0722 - val_accuracy: 0.9030 - val_loss: 0.2827\n",
      "Epoch 5/10\n",
      "\u001b[1m7505/7505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m921s\u001b[0m 123ms/step - accuracy: 0.9758 - loss: 0.0692 - val_accuracy: 0.9030 - val_loss: 0.2856\n",
      "Epoch 6/10\n",
      "\u001b[1m7505/7505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m923s\u001b[0m 123ms/step - accuracy: 0.9767 - loss: 0.0646 - val_accuracy: 0.9044 - val_loss: 0.2840\n",
      "Epoch 7/10\n",
      "\u001b[1m7505/7505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m925s\u001b[0m 123ms/step - accuracy: 0.9779 - loss: 0.0638 - val_accuracy: 0.9066 - val_loss: 0.2770\n",
      "Epoch 8/10\n",
      "\u001b[1m7505/7505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m926s\u001b[0m 123ms/step - accuracy: 0.9787 - loss: 0.0621 - val_accuracy: 0.9030 - val_loss: 0.3074\n",
      "Epoch 9/10\n",
      "\u001b[1m7505/7505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m927s\u001b[0m 124ms/step - accuracy: 0.9792 - loss: 0.0615 - val_accuracy: 0.9066 - val_loss: 0.2808\n",
      "Epoch 10/10\n",
      "\u001b[1m7505/7505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m930s\u001b[0m 124ms/step - accuracy: 0.9787 - loss: 0.0598 - val_accuracy: 0.9055 - val_loss: 0.3055\n"
     ]
    }
   ],
   "source": [
    "history_extended_data_base_model = model.fit(\n",
    "    training_dataset,\n",
    "    validation_data = validation_dataset,\n",
    "    epochs = 10,\n",
    "    verbose = 1,\n",
    "    callbacks = [EarlyStoppingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "e055082f-b195-44a9-a07a-ae68a9fd2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:100]:  # Freeze first 100 layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile and train the model again\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "5a37aae4-f3e7-4dda-be74-863f4bceb065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_8      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_8      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │         \u001b[38;5;34m5,120\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,055,972</span> (15.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,055,972\u001b[0m (15.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,844,189</span> (14.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,844,189\u001b[0m (14.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,783</span> (827.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m211,783\u001b[0m (827.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ac0c8-b307-4b9c-81ef-ff152bf475c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2591/7505\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m24:08\u001b[0m 295ms/step - accuracy: 0.9570 - loss: 0.1037"
     ]
    }
   ],
   "source": [
    "history_extended_data_model_tr = model.fit(\n",
    "    training_dataset,\n",
    "    validation_data = validation_dataset,\n",
    "    epochs = 10,\n",
    "    verbose = 1,\n",
    "    callbacks = [EarlyStoppingCallback(), checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "616bc549-313e-4806-87a0-5f354cb19fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/Users/sergeiakhmadulin/Documents/Cleaned/model_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "196c3f9a-e8b6-4644-9b67-d79464d68359",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_test[\"label\"] = np.where(results_df_test[\"probs\"]>=0.5,\"dirty\",\"cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1add5722-03c5-4696-875a-f283c557e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_test = results_df_test.iloc[:,:2]\n",
    "results_df_test['id'] = results_df_test['id'].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dfa15913-48fc-4d45-b34a-96b90ab51b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0071</td>\n",
       "      <td>cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0717</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0703</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0065</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0059</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>0040</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>0726</td>\n",
       "      <td>cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0732</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>0054</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>0068</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>744 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    label\n",
       "0    0071  cleaned\n",
       "1    0717    dirty\n",
       "2    0703    dirty\n",
       "3    0065    dirty\n",
       "4    0059    dirty\n",
       "..    ...      ...\n",
       "739  0040    dirty\n",
       "740  0726  cleaned\n",
       "741  0732    dirty\n",
       "742  0054    dirty\n",
       "743  0068    dirty\n",
       "\n",
       "[744 rows x 2 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "703e43d6-e1c6-4291-b615-fbaf783e5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_test.to_csv('/Users/sergeiakhmadulin/Documents/Cleaned/submission_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398dbd7-3078-4cc8-a96d-3a17d6c86357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
